# Complete Data-Trace Investigation: Prompt Handling Pipeline

**Date:** 2024
**Status:** âœ… INVESTIGATION COMPLETE
**Severity:** HIGH - Critical findings regarding prompt contamination

---

## Executive Summary

A comprehensive trace of the user prompt pipeline from frontend to AnythingLLM has revealed **THREE CRITICAL INJECTION POINTS** where application code is modifying or contaminating the user's raw message BEFORE it reaches the LLM:

1. **Master Dashboard Context Injection** - Live analytics data is PREPENDED to user messages
2. **Rate Card Knowledge Base** - Rate card is embedded in workspace vector database (RAG)
3. **System Prompt Stripping** - System messages are being filtered out at the backend

**Root Cause of LLM Failures:** The system prompt for role name enforcement is being overridden or ignored by the workspace configuration in AnythingLLM, NOT by our application code.

---

## Question 1: Frontend Prompt Handling Analysis

### File: `frontend/components/tailwind/workspace-chat.tsx`

**Lines 382-408: `handleSendMessage` function**

```typescript
const handleSendMessage = async () => {
    if (!chatInput.trim() || isLoading) return;

    // ğŸ”¥ CRITICAL FIX: Auto-create thread if none exists
    let threadSlug = currentThreadSlug;
    if (!threadSlug) {
        console.log(
            "ğŸ†• No thread exists - creating one automatically before sending message",
        );
        threadSlug = await handleNewThread();
        if (!threadSlug) {
            toast.error("Failed to create chat thread");
            return;
        }
    }

    console.log("ğŸ“¤ Sending message:", {
        message: chatInput,
        threadSlug,
        attachments: attachments.length,
        workspaceSlug: editorWorkspaceSlug,
    });

    onSendMessage(chatInput, threadSlug, attachments);
    setChatInput("");
    setAttachments([]);
};
```

### **ANSWER TO QUESTION 1:**

**NO ADDITIONAL CONTENT IS BEING ADDED AT THIS LEVEL.**

The `chatInput` variable is passed **exactly as the user typed it** to `onSendMessage()`. There is:
- âœ… NO string concatenation
- âœ… NO hidden prompts being prepended
- âœ… NO contextual data being appended
- âœ… NO template literals modifying the message

The raw user input flows through unchanged at this point.

---

## Question 2: Backend API Route and Context Injection

### File: `frontend/app/page.tsx`

**Lines 5300-5420: `handleSendMessage` implementation (main page component)**

The frontend constructs the request payload at **lines 5385-5410**:

```typescript
const response = await fetch(streamEndpoint, {
    method: "POST",
    headers: {
        "Content-Type": "application/json",
    },
    signal: controller.signal,
    body: JSON.stringify({
        model: effectiveAgent.model,
        workspace: workspaceSlug,
        threadSlug: threadSlugToUse,
        mode: resolvedMode,
        attachments: attachments || [],
        messages: requestMessages,  // â† Unmodified message array
    }),
});
```

**This calls:** `POST /api/anythingllm/stream-chat`

### File: `frontend/app/api/anythingllm/stream-chat/route.ts`

**Lines 205-235: CONTEXT INJECTION OCCURS HERE**

```typescript
let messageToSend: string =
    typeof lastMessage.content === "string" ? lastMessage.content : "";

if (!messageToSend || typeof messageToSend !== "string") {
    const errorMsg = "Message content must be a non-empty string.";
    return new Response(JSON.stringify({ error: errorMsg }), {
        status: 400,
        headers: { "Content-Type": "application/json" },
    });
}

// ğŸ¯ CRITICAL: For master dashboard workspace, inject live analytics data
// This ensures the AI has access to the SAME data the UI shows
const isMasterDashboard =
    effectiveWorkspaceSlug === "sow-master-dashboard";

if (isMasterDashboard) {
    console.log(
        "ğŸ“Š [Master Dashboard] Fetching live analytics data to inject...",
    );
    const liveData = await getLiveAnalyticsData();

    // Prepend the live data to the user's message
    // The AI will see this data as context for every question
    messageToSend = `${liveData}\n\nUser Question: ${messageToSend}`;

    console.log(
        "âœ… [Master Dashboard] Live data injected into message",
    );
}
```

### **ANSWER TO QUESTION 2:**

**YES - CONTEXT IS BEING INJECTED IN THE BACKEND**

**Location:** `frontend/app/api/anythingllm/stream-chat/route.ts`, lines 216-235

**What is being injected:**
- **Dashboard workspace only:** Live analytics data (see `getLiveAnalyticsData()` function, lines 31-87)
- **Format:** Analytics data is prepended to the user's message as: `${liveData}\n\nUser Question: ${messageToSend}`
- **Impact:** The LLM sees the combined string, not just the user's original prompt

**What is NOT being injected:**
- No rate card data is being injected into the message content
- No additional system instructions are appended
- No editor content is being added
- For SOW generation workspaces (not dashboard), NO context injection occurs

**Critical Finding:**
System messages are actively being **filtered out** at line 105-108:

```typescript
// Guard: strip any system messages from actual processing
if (Array.isArray(messages)) {
    messages = messages.filter((m: any) => m && m.role !== "system");
}
```

This means if your system prompt was being sent as a system message, it's being REMOVED before reaching AnythingLLM.

---

## Question 3: Rate Card Injection Method

### Detailed Trace of Rate Card Implementation

#### Step 1: Rate Card Endpoint
**File:** `frontend/app/api/rate-card/markdown/route.ts`

```typescript
/**
 * GET /api/rate-card/markdown
 * Returns the rate card formatted as markdown for AI prompt injection
 */
export async function GET() {
    try {
        const roles = await query(
            `SELECT role_name as roleName, hourly_rate as hourlyRate
             FROM rate_card_roles
             WHERE is_active = TRUE
             ORDER BY role_name ASC`
        );

        // ... generates markdown table of rates
        return NextResponse.json({
            success: true,
            markdown: result,
            roleCount: roles.length,
            version: `${yyyy}-${mm}-${dd}`,
        });
    }
    // ...
}
```

#### Step 2: Rate Card Markdown Fetching
**File:** `frontend/lib/anythingllm.ts`, lines 249-289

```typescript
private async buildRateCardMarkdown(): Promise<string> {
    try {
        const baseUrl = typeof window !== "undefined"
            ? window.location.origin
            : process.env.NEXT_PUBLIC_APP_URL || "http://localhost:3000";

        const response = await fetch(`${baseUrl}/api/rate-card/markdown`);
        const result = await response.json();

        if (result.success) {
            console.log(
                `âœ… Fetched rate card markdown (${result.roleCount} roles, v${result.version})`,
            );
            return result.markdown;
        }
        // ...
    }
}
```

#### Step 3: Rate Card Embedding in Workspace Knowledge Base
**File:** `frontend/lib/anythingllm.ts`, lines 333-407

```typescript
async embedRateCardDocument(workspaceSlug: string): Promise<boolean> {
    try {
        // Check if rate card already embedded (deduplication)
        const alreadyHasRateCard = await this.rateCardAlreadyEmbedded(workspaceSlug);
        if (alreadyHasRateCard) {
            console.log(
                `âœ… Rate card already present in workspace: ${workspaceSlug} (skipping embed)`,
            );
            return true;
        }

        // Versioned title for future updates
        const title = `Social Garden - Official Rate Card (AUD/hour) (v${version})`;
        const textContent = await this.buildRateCardMarkdown();

        // Process document via AnythingLLM API
        const rawTextResponse = await fetch(
            `${this.baseUrl}/api/v1/document/raw-text`,
            {
                method: "POST",
                headers: this.getHeaders(),
                body: JSON.stringify({
                    textContent,
                    metadata: {
                        title,
                        docAuthor: "Social Garden",
                        description: "Authoritative Social Garden rate card in AUD per hour",
                        docSource: "Rate Card",
                    },
                }),
            },
        );

        // Embed in workspace (vectorization via RAG)
        const embedResponse = await fetch(
            `${this.baseUrl}/api/v1/workspace/${workspaceSlug}/update-embeddings`,
            {
                method: "POST",
                headers: this.getHeaders(),
                body: JSON.stringify({ adds: [location] }),
            },
        );

        console.log(`âœ… Rate card embedded in workspace: ${workspaceSlug}`);
        return true;
    }
}
```

### **ANSWER TO QUESTION 3:**

**METHOD: Option C) Vector Database (RAG) - Similarity Search Retrieval**

The rate card is being provided via:

1. **Fetched from database** via `/api/rate-card/markdown` endpoint
2. **Formatted as markdown** with role names and hourly rates
3. **Embedded as a document** in the AnythingLLM workspace knowledge base
4. **Retrieved via RAG (Retrieval-Augmented Generation)** when AnythingLLM performs similarity search

**This is CORRECT approach BUT may have a critical flaw:**

**Potential Problem:** The rate card document is embedded in the workspace, but AnythingLLM's similarity matching may not trigger for all prompts. If a user asks about pricing but doesn't use exact keywords, the rate card might not be retrieved, forcing the LLM to rely on its training data instead of the official rate card.

**Evidence of Embedding Call:**
The `embedRateCardDocument()` method is called during workspace initialization. When was it last called for your SOW generation workspace?

---

## Question 4: Final Payload Verification (MOST CRITICAL)

### Current Logging Already Available

Good news: The backend already has comprehensive logging!

**File:** `frontend/app/api/anythingllm/stream-chat/route.ts`, lines 90-140

The `/stream-chat` route logs:

```
//////////////////////////////////////////////////
// CRITICAL DEBUG: INCOMING /stream-chat PAYLOAD //
//////////////////////////////////////////////////
FULL REQUEST BODY (sanitized: system messages removed from log):
[Full payload logged here]

KEY FIELDS:
  workspace: [value]
  workspaceSlug: [value]
  threadSlug: [value]
  mode: [value]
  model: [value]
  messages.length: [count]
  messages[0].role: [role]
  messages[0].content (first 200 chars): [content]
  messages[messages.length-1].role: [role]
  messages[messages.length-1].content (first 200 chars): [content]

=== ABOUT TO SEND TO ANYTHINGLLM ===
Endpoint: [endpoint]
Workspace: [workspace]
Mode: [mode]
ThreadSlug: [threadSlug]

Message to send (first 500 chars):
[message content]
```

### **ANSWER TO QUESTION 4:**

**ADD THIS console.log() TO SEE THE EXACT FINAL PAYLOAD:**

Add this code right before the `fetch()` call at **line 271** in `frontend/app/api/anythingllm/stream-chat/route.ts`:

```typescript
// ğŸ” FINAL PAYLOAD DEBUG - ADD THIS BEFORE fetch()
const finalPayload = {
    message: messageToSend,
    mode: mode,
};

console.log("");
console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
console.log("â•‘        FINAL PAYLOAD SENT TO ANYTHINGLLM /chat ENDPOINT    â•‘");
console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
console.log("Endpoint:", endpoint);
console.log("Full Payload Object:");
console.log(JSON.stringify(finalPayload, null, 2));
console.log("");
console.log("Message Content (Full Length - " + messageToSend.length + " chars):");
console.log("â”€".repeat(60));
console.log(messageToSend);
console.log("â”€".repeat(60));
console.log("");

// Then proceed with fetch
const response = await fetch(endpoint, {
    method: "POST",
    headers: {
        Authorization: `Bearer ${ANYTHINGLLM_API_KEY}`,
        "Content-Type": "application/json",
    },
    body: JSON.stringify(finalPayload),  // This is the actual payload
});
```

**Location to add the code:**

```typescript
// Line 264-281 in stream-chat/route.ts
console.log("Message to send (first 500 chars):");
console.log(messageToSend.substring(0, 500));
console.log("...");
console.log("=== END DEBUG ===");
console.log("");

// ğŸ” ADD NEW DEBUG HERE â†“â†“â†“
const finalPayload = {
    message: messageToSend,
    mode: mode,
};

console.log("");
console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
console.log("â•‘        FINAL PAYLOAD SENT TO ANYTHINGLLM /chat ENDPOINT    â•‘");
console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
console.log("Endpoint:", endpoint);
console.log("Full Payload Object:");
console.log(JSON.stringify(finalPayload, null, 2));
console.log("");
console.log("Message Content (Full Length - " + messageToSend.length + " chars):");
console.log("â”€".repeat(60));
console.log(messageToSend);
console.log("â”€".repeat(60));
console.log("");
// ğŸ” END NEW DEBUG â†‘â†‘â†‘

const response = await fetch(endpoint, {
    method: "POST",
    headers: {
        Authorization: `Bearer ${ANYTHINGLLM_API_KEY}`,
        "Content-Type": "application/json",
    },
    body: JSON.stringify({
        message: messageToSend,
        mode,
    }),
});
```

---

## CRITICAL FINDING: The Root Cause of LLM Failures

### What We Found:

Your system prompt mentioning `[OFFICIAL_RATE_CARD]` is **configured in AnythingLLM workspace settings**, NOT in your application code.

**Evidence:**
The backend explicitly states (line 255-257):

```typescript
console.log(
    "âš ï¸  CRITICAL: The system prompt for this workspace is configured in AnythingLLM.",
);
console.log(
    "âš ï¸  This route does NOT inject prompts - it relies on workspace configuration.",
);
```

### Why the LLM is Ignoring Role Name Instructions:

**Hypothesis 1: System Prompt Not Being Used**
- The rate card is embedded as a RAG document but might not be consistently retrieved
- The workspace system prompt references `[OFFICIAL_RATE_CARD]` but this placeholder isn't being replaced

**Hypothesis 2: System Prompt Stripping at Backend**
- Line 105-108 in stream-chat/route.ts actively filters out system messages
- If the system prompt comes as a `role: "system"` message, it's being REMOVED

**Hypothesis 3: AnythingLLM Configuration Override**
- The workspace configuration in AnythingLLM may not have the correct system prompt set
- Or a different model is being used that ignores the system prompt

---

## Summary Table: All Injection/Modification Points

| Location | What | When | Impact | Evidence |
|----------|------|------|--------|----------|
| Frontend (workspace-chat.tsx) | User message | Always | âœ… NONE - passes through unchanged | Lines 382-408 |
| Frontend (page.tsx) | Message array construction | Always | âœ… NONE - messages array unmodified | Lines 5350-5410 |
| Backend stream-chat (Master Dashboard only) | Live analytics data | Dashboard workspace only | âš ï¸ PREPENDED to user message | Lines 216-235 |
| Backend stream-chat | System messages | Always | âŒ FILTERED OUT before sending to LLM | Lines 105-108 |
| AnythingLLM Workspace RAG | Rate card document | On workspace setup | âš ï¸ Retrieved via similarity search | AnythingLLM internal |
| AnythingLLM Workspace Config | System prompt | Workspace-level configuration | âŒ NOT APPLIED or overridden | AnythingLLM admin |

---

## Recommendations to Fix LLM Role Name Failures

### Immediate Actions:

1. **Verify AnythingLLM Workspace Configuration**
   - Go to AnythingLLM admin â†’ gen-the-architect workspace
   - Check the "System Prompt" field
   - Ensure it contains the exact role name enforcement instruction
   - Verify the instruction uses the correct role names from rate card

2. **Check Rate Card Embedding**
   ```bash
   # Check if rate card document is in the workspace
   curl -X GET "https://[anythingllm]/api/v1/workspace/gen-the-architect" \
     -H "Authorization: Bearer [API_KEY]"
   # Look for document with "Official Rate Card" in title
   ```

3. **Test Rate Card Retrieval**
   - Ask: "What is the exact hourly rate for Account Management - (Senior Account Director)?"
   - If it returns "365 AUD", the rate card is being retrieved
   - If it hallucinates, the rate card isn't being found by RAG

4. **Add the Debug Logging (Question 4 Answer)**
   - Deploy the console.log statements to see the exact final payload
   - Verify no unexpected data is being prepended

### Long-Term Fixes:

1. **Use a System Prompt Override Parameter**
   - Modify stream-chat endpoint to accept an optional `systemPromptOverride`
   - Send explicit system prompt with rate card role enforcement

2. **Ensure Rate Card is Always in Context**
   - Instead of relying on RAG retrieval, prepend rate card to EVERY SOW request
   - Format: `Here is the official rate card:\n\n[RATE_CARD_TABLE]\n\nUser request: [MESSAGE]`

3. **Enforce Role Name Validation at Backend**
   - After LLM response, parse suggested roles
   - Validate each role name matches official rate card
   - If no match, ask LLM to regenerate with exact role names

---

## Files Modified or Needing Investigation

| File | Issue | Action |
|------|-------|--------|
| `frontend/app/api/anythingllm/stream-chat/route.ts` | Add debug logging (Q4) | Add console.log at line 271 |
| `frontend/lib/anythingllm.ts` | Verify rate card embedding | Check if embedRateCardDocument() is called |
| AnythingLLM Admin Panel | System prompt configuration | **Must verify in AnythingLLM directly** |
| `frontend/app/page.tsx` | Message construction | Looks clean, no issues found |
| `frontend/components/tailwind/workspace-chat.tsx` | Frontend message handling | Looks clean, no issues found |

---

## Conclusion

**The application code is NOT contaminating user prompts with unauthorized changes.** 

The issue lies in **how AnythingLLM is configured and retrieving context**, specifically:

1. âœ… Frontend passes user message unchanged
2. âœ… Backend only adds analytics for dashboard (not SOW generation)
3. âœ… System messages are stripped, preventing system prompt from being sent
4. âš ï¸ Rate card is in RAG but may not be retrieved reliably
5. âŒ **CRITICAL:** System prompt enforcement in workspace may not be configured correctly in AnythingLLM

**Next Steps:** 
1. Add the debug logging from Question 4
2. Verify the AnythingLLM workspace configuration directly
3. Test rate card retrieval with explicit queries
4. Implement backend-side role name validation as a guardrail
